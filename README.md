# Repository for KG construction using LLM
This is the official repository for KG construction using LLM.  

## Requirements
Set up a virtual environment and install the required packages.
```sh
pdm install
```
Note that, if you want to run our baseline models, which is AllenNLP, you need to install another virtual environment because the packages are not compatible. 
```sh
cd allen
pdm install
```

Speeding up by using Flash Attention if your device support it. 
```sh
pip install -U flash-attn --no-build-isolation
```

Download models for easier usage.  
en_core_sci_sm: https://allenai.github.io/scispacy/

## Data preparation 
### Step1: Generate `triples.txt` and `triples_w_type.txt` from metadata

Run the following command to generate `triples.txt` from metadata for **UMLS** and **iBKH** since they are originally triples data with simple relationships.  
```sh
python data_processing/triples2triples.py
```

Run the following command to generate `triples.txt` and `triples_w_type.txt` from metadata for **PrimeKG**, **BATMAN-TCM**, **TCMBank**, **CKG**, since they are originally table data with complex relationships.   
```sh
python data_processing/tables2triples.py
```

### Step2: Filter `triples.txt`

For improving recall, you can try to add custom fitering rules to exclude some entities or relations. For example,   
* Exclude “text_semantic” or “inferred” relations  
* Exclude Molecule entities  
* Exclude very long entities that over 500 characters  
* Exclude decimal entities  
* Exclude/translate non-English entities  

In some cases, we only use drug, gene, disease, symptoms, side effects, pathways and relations among them. You may also exclude all information from the CTD database.

Run the following command to filter `triples.txt`:   
```sh
python data_processing/filter_triples.py
```

### Step3: Generate necessary keyword files used to retrieve documents and examples files to instruct llm models for generating KG triples based on freqency of entities and relations. As an option, you can also generate/refine keyword files and example files using LLM models.   

We use keywords file to retrieve documents from PubMed and other data sources. Specifically, we use the following files:   
`h_r_t.txt`, `h_r_t_w_type.txt`, which contains all the entities and relations in the `triples.txt` and `triples_w_type.txt` file.  
`hr_rt_ht.txt`, `hr_rt_ht_w_type.txt`, which contains all the head-relation/relation-tail/head-tail pairs in the `triples.txt` and `triples_w_type.txt` file.  
We also has ablation experiments to use `triples.txt` and `triples_w_type.txt` file dicretly as the keyword file. 

We use `criteria.json` file to instruct the llm models to generate triples. The `criteria.json` file contains the following example files:   
`purposes.txt`, `purposes_mistral.txt`, `purposes_llama.txt`, which contains the purpose/motivation of creating/extending the KG, manufactured by human.  
`entity_types.txt`, `entity_types_mistral.txt`, `entity_types_llama.txt`, which contains the all the entity types in the `triples.txt` or `triples_w_type.txt` file that we want to include in the extended KG.   
`an_example.txt`, which contains an example of a triple in the `triples.txt` file.   
`more_examples.txt`, which contains more examples of triples in the `triples.txt` file.   
`h_t.txt`, which contains part of the entities in the `triples.txt` file, since it is not possible to list all the entities in the prompt.    
`r.txt`, which contains the all the relations in the `triples.txt` file that we want to include in the extended KG. 

Run the following command to generate the necessary keyword files and example files:   
```sh
python data_processing/extract_freq_keywords_and_examples.py
```

_Note: If processing other datasets_

_Above steps usually don't take long time and can be done on CPU. So, it is recommended to run this step in an interactive terminal so that you can see if there are any errors or warnings in processing the data. Although our pipeline is designed to handle a lot of kind of data, i.e., both triples and tables, some medical data sources may include complex symbols that cause errors or warnings to process them into our standard triples._ 

As an option, you can run the following command to generate/refine keyword files and example files using LLM models:   
```sh
python data_processing/mistral_generate_keywords_and_examples.py  
python data_processing/llama_generate_keywords_and_examples.py  
```

### Step4: Generate and evaluate KG triples using the generated keyword files and example files  

#### Run the experiment and evaluate the KG triples generated by the LLM models.  
Using this repository, we desinged two ways to get source articles for generating KG triples: `fetch` and `read`. The `fetch` mode uses the `entrezFetcher.py` file to fetch articles from entrez database, and the `read` mode directly reads the local articles in the `data/sources/` directory. We recommend `fetch` mode if you want to get the articles from entrez database, and `read` mode if you want to use your local articles. 

Run the following command to fetch articles from entrez database and generate and evaluate KG triples, based on the generated keyword files and example files:   
```sh
python main.py --model mistral --evaluator mistral --keywords_expander none --keywords_file h_r_t.txt --article_acquisition_mode fetch --dataset UMLS --log_file F_log --retrieval_method bm25s --retmax 5 --top_k 3 --iterations 3 --dbs pubmed --save_dir output 
```

Run the following command to read local articles and generate and evaluate KG triples, based on the generated keyword files and example files:   
```sh
python main.py --model llama --evaluator mistral --keywords_expander none --keywords_file h_r_t.txt --article_acquisition_mode read --read_articles_file pubmed.json --dataset UMLS --log_file R_log --retrieval_method bm25s --top_k 3 --iterations 3 --save_dir output 
```

#### Additional GPT4/Gemini evaluation
**Model** 
In our current settings, you can use the following models as generator and expander:    
* [Phi-3.5-mini](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) | Context Length: 4,096 tokens.    
* [Llama-3.1-405B](https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct) | Context Length: Typically up to 4,096 tokens for Llama models.   
* [Llama-3-70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | Context Length: Typically up to 4,096 tokens. 
* [Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) | Context Length: Typically up to 4,096 tokens.    
* [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) | Context Length: Up to 8,192 tokens.    
* [GPT-3.5-turbo](https://platform.openai.com/docs/models/o1) | Context Length: Up to 4,096 tokens    
* [GPT-4o-mini](https://platform.openai.com/docs/models/gpt-4o-mini) | Context Length: Typically up to 4,096 tokens      
* [GPT-4o](https://platform.openai.com/docs/models/gpt-4o) | Context Length: Up to 8,192 tokens     
* [Gemini](https://ai.google.dev/) - [TODO]: check the context length -     

_Note: A token is approximately 4 characters of text in English. So, 1,000 tokens correspond to about 750 words._

#### Note: For running large scale of iterations 

When running large scale of iterations, we recommend to split the keyword files into multiple sub-files, and run multiple processes, each process processing one sub-file.  
To do this, first run the `txt_split.py` file to split the keyword files into multiple sub-files.  
```sh
python data_processing/txt_split.py
```
Then, use the sub-files as keyword files to run the program. After running the program, the results file can be integrated using the `txt_integrate.py` file.  
```sh
python data_processing/txt_integrate.py
```

## Time and Memory Cost Per keyworded Iteration
Using GPU: A100-40GB

retrieval_method="rankbm25"
retmax=30   # Number of articles to fetch each time (don't set > 10)
top_k=30    # Number of documents to retrieve each time

Model | Context Length | Memory Cost | Time for Generation | Time for Evaluation
--- | --- | --- | --- | ---
Llama-3.1-405B | 4096 | ? GB | ? minutes | ? minutes
Llama-3-70B | 4096 | 38 GB | 5-10 minutes | 1-2 minutes
Llama-3-8B | 4096 | ? GB | 2-5 minutes | 1-2 minutes
Mistral-7B | 4096 | ? GB | 35 minutes | 35 minutes  
GPT-3.5-turbo | - | - | 1 minutes | 1 minutes
GPT-4o-mini | - | - | 1 minutes | 1 minutes
GPT-4o | - | - | 1 minutes | 1 minutes
Gemini | - | - | 1 minutes | 1 minutes

## Neo4j Deployment

Download and install [Neo4j](https://neo4j.com) Desktop from https://neo4j.com/download/